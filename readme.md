# TeethDreamer: 3D Teeth Reconstruction from Five Intra-oral Photographs
Chenfan Xu*, [Zhentao Liu](https://zhentao-liu.github.io/)*, Yuan Liu, Yulong Dou, Jiamin Wu, Jiepeng Wang, Minjiao Wang, [Dingggang Shen](https://idea.bme.shanghaitech.edu.cn/), and [Zhiming Cui](https://shanghaitech-impact.github.io/)<sup>+</sup>.

## [[Paper](https://arxiv.org/abs/2407.11419)]|[[Project Page](https://shanghaitech-impact.github.io/TeethDreamer/)]

- [x] Inference code and pretrained models.
- [x] Training code.
- [x] [Rendering dataset](https://pan.baidu.com/s/1uX-JB4GCKdtOBjYbkrFylg?pwd=wuhp) with 16 views for each case.

### Demo
- [x] Five intra-oral photos in `example/oral`.
- [x] Segmented teeth images in `example/teeth`.
- [x] Generated [images](https://shanghaitecheducn-my.sharepoint.com/:f:/g/personal/xuchf2023_shanghaitech_edu_cn/EsjO23PTzMtJmk-XpkMh01oBEfaui-ZZ_5afbU1t9Ijc6g?e=4aG5ZS) and [Reconstruction](https://shanghaitecheducn-my.sharepoint.com/:f:/g/personal/xuchf2023_shanghaitech_edu_cn/EuqQwcmmWH9NsdaeDYnO17kB7z7PDRP_YLzQ_cm-OOQxDQ?e=F2AigU).

### Getting started
1. Install packages in `requirements.txt`. We test our model on a 40G A100 GPU with 11.6 CUDA and 1.12.0 pytorch. 
```angular2html
conda create -n TeethDreamer
conda activate TeethDreamer
pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu116
pip install -r requirements.txt
```
2. Download pretrained model [checkpoints](https://shanghaitecheducn-my.sharepoint.com/:f:/g/personal/xuchf2023_shanghaitech_edu_cn/Em8U10EGc09Om0SQKA-s6WoBiYX9hkm-0BWiReTFnwIsuQ?e=6vb0Xl)
### Inference
1. Make sure you have the following models.
```bash
SyncDreamer
|-- ckpt
    |-- ViT-L-14.ckpt
    |-- TeethDreamer.ckpt
    |-- zero123-xl.ckpt
    |-- sam_vit_b_01ec64.pth
```
2. Segment upper and lower teeth from your five intral-oral photos. You can use our script `seg_teeth.py` in the command line. Before that, you need renumber your five intral-oral images by 0~4 corresponding to anterior view, left buccal view, right buccal view, maxillary occlusal view, and mandibular occlusal view. And then you can manually segment upper teeth and lower teeth by clicking left mouse button in the target area within the interactive interface created by our script.

Tips: You can refer to image files in `example` folder.
```angular2html
python seg_teeth.py --img directory/of/your/intra-oral/images \
                    --seg directory/to/store/segmented/images \
                    --suffix suffix/of/your/image/files
```
Tips: You need segment upper teeth for the first four intra-oral images and then lower teeth for the last four images. If unexpected regions are segmented, you can click the right mouse button to label the irrelevant area.

3. Make sure you have following file structure.
```bash
|-- your_seg_dir
    |-- XXX_norm_lower
    |-- XXX_norm_upper
```

4. Generate color and normal images of eight viewpoints.
```angular2html
python TeethDreamer.py -b configs/TeethDreamer.yaml \
                       --gpus 0 \
                       --test ckpt/TeethDreamer.ckpt \
                       --output directory/to/store/generated/images \
                       data.params.test_dir=directory/of/segmented/images
```
5. (Optional) Segment foreground of the generated image manually which is necessary for Neus (Because the foreground mask automatically generated by `rembg` package may be wrong sometimes)
```angular2html
python seg_foreground.py --img path/to/your/generated/image \
                         --seg path/to/your/segmented/image \
```
6. Reconstruct tooth model from generation by Neus.
```angular2html
cd instant-nsr-pl
python run.py --img ../example/results/generation/1832_upper_cond_000_000_000_000.png" \
              --cpu 4 \
              --dir ../example/results/reconstruction/ \
              --normal \
              --rembg
```
Explanation: 
- `--img` is the path to your generated image
- `--cpu` is the number of your CPU cores
- `--dir` is the directory to store reconstruction
- `--normal` indicates the generation includes normal images
- `--rembg` indicates background removement (The foreground mask is necessary here)

### Data Preparation for Training
1. Make sure you have normalized tooth models which are segmented from intra-oral scanning models.
2. Render color and normal images by blender scripts. We test following scripts on Windows with Blender 4.0.1
```angular2html
blender --background
        --python normal_render.py
        -- --object_path path/to/your/tooth/model
        --target_dir directory/to/store/rendered/normal/images
        --input_dir directory/to/store/rendered/condition/images
```
```angular2html
blender --background
        --python color_render.py
        -- --object_path path/to/your/tooth/model
        --target_dir directory/to/store/rendered/color/images
        --input_dir directory/to/store/rendered/condition/images
```
Explanation: 
`normal_render.py` only renders target normal images with fixed viewpoints set by `view16` dictionary.
`color_render.py` renders condition images corresponding to segmented intra-oral photos taken by dentists and target color images.
3. Make sure you have a `pkl` file which includes a dictionary with `train`, `val` keys and corresponding lists including cases' ids such as `XXX_norm_lower` and `XXX_norm_upper`.
4. Check if your directory of rendering data has following structures.
```bash
Data
|-- target
|-- normal
|-- input
|-- mv-splits.pkl
```
Explanation: 
The `target` folder is the directory of your rendered color images which is the argument `target_dir` in the script `color_render.py`.

The `normal` folder is the directory of your rendered normal images which is the argument `target_dir` in the script `normal_render.py`.

The `input` folder is the directory of your rendered condition images which is the argument `input_dir` in the script `color_render.py`.

The `mv-splits.pkl` file is the `pkl` file mentioned in the previous step.
5. Finetune pretrained zero123 model on your own data.
```angular2html
python TeethDreamer.py -b configs/TeethDreamer.yaml \
                       --gpus 0 \
                       --finetune_from ckpt/zero123-xl.ckpt \
                       data.target_dir=path/to/your/target/folder \
                       data.inpt_dir=path/to/your/input/folder \
                       data.uid_set_pkl=path/to/your/pkl/file \
                       data.validation_dir=path/to/your/input/folder
```

## Acknowledgement










Hardware configuration currently - - rtx 4060 gpu  , 24 gb ram , i7 13650 HX processor

software toolkit -- 
1. Version Control: git (with Shallow Cloning)
What it is: The tool used to download the TeethDreamer code from GitHub.

Why we are using it: It is the standard way to get repository code.

Our Minimal Strategy: Normally, git clone downloads the entire history of every change ever made to the project. We will use a flag called --depth 1 (a "shallow clone"). This tells Git to only download the absolute newest version of the files, skipping the gigabytes of historical data.

2. Environment Manager: python3-venv
What it is: Python's built-in tool for creating isolated environments.

Why we are using it: If you install AI packages directly to your main Ubuntu system, you risk breaking your OS. A venv keeps the project's dependencies locked inside a single folder.

Our Minimal Strategy: Many AI tutorials recommend Anaconda (Conda). We are completely avoiding Conda. Conda installs a massive base environment that takes up gigabytes of space before you even download your first AI model. venv is built-in, lightweight, and takes up virtually zero space on its own.

3. Package Installer: pip (with Cache Disabled)
What it is: The official installer for Python packages. We use it to read TeethDreamer's requirements.txt file and download what it needs.

Our Minimal Strategy: By default, when pip downloads a 2.5 GB AI library like PyTorch, it saves a hidden backup copy in a cache folder just in case you want to install it again later. This instantly doubles your storage usage. We will use the --no-cache-dir command. It forces pip to download, install, and immediately delete the installation file.

4. Deep Learning Framework: PyTorch (CUDA 12.1)
What it is: The engine that runs the AI math. TeethDreamer is built on it.

Why we are using it: We have to, but we must install the specific version built for your hardware.

Our Minimal Strategy: Your RTX 4060 uses Nvidia's Ada Lovelace architecture. It requires a modern version of CUDA (Nvidia's parallel computing platform) to run efficiently. We will install the PyTorch version specifically compiled for CUDA 12.1. We will also install only the GPU version, ensuring we don't accidentally download bloated, unnecessary CPU-only backups.

5. Memory Optimizer: xformers
What it is: A library developed by Meta that optimizes how AI models calculate "attention."

Why we are using it: While you have plenty of system RAM (24GB), your RTX 4060 only has 8GB of VRAM (Video RAM). Generating 3D assets is incredibly VRAM-heavy. Without optimization, an 8GB card will often hit an "Out of Memory" (OOM) error and crash. xformers heavily reduces VRAM usage during generation, acting as a crucial safety net for your 4060.




build-essential & ninja-build: TeethDreamer relies on Neural Surface Reconstruction. These algorithms often compile custom CUDA kernels directly on your machine during setup. We need standard C++ compilers to allow this.

libgl1-mesa-glx & libglib2.0-0: The repository uses an interactive tool (seg_teeth.py) and OpenCV to process images. Without these Linux graphics libraries, OpenCV will instantly crash when trying to open or read an image.




Here is the comprehensive, step-by-step guide to replicating your entire high-performance ML environment. This is written in a professional GitHub-style format, suitable for a `README.md` or a personal documentation wiki.

---

# üöÄ Ultra-Performance ML Setup: RTX 4060 + CUDA 13.1 + PyTorch

> **Project Focus:** 3D Teeth Reconstruction from Images & NVIDIA Isaac Sim
> **The Goal:** Run cutting-edge Neural Graphics Primitives (NGP) with forward-compatible drivers.

---

## üèó Phase 1: The Foundation (Driver & CUDA)

To support the **Ada Lovelace** architecture of the **RTX 4060**, we use the latest 590+ driver branch.

1. **System Cleanse:** Remove any conflicting legacy CUDA toolkits.
```bash
sudo apt-get --purge remove "*cublas*" "*cufft*" "*curand*" "*cusolver*" "*cusparse*" "*npp*" "*nvgpu*" "cuda*" "nvidia*"
sudo apt autoremove

```


2. **Install CUDA 13.1:** Download the local runfile from NVIDIA.
```bash
wget https://developer.download.nvidia.com/compute/cuda/13.1.0/local_installers/cuda_13.1.0_590.48_linux.run
sudo sh cuda_13.1.0_590.48_linux.run

```


*Select "Install" and ensure the Driver (590.48) and Toolkit are checked.*
3. **Environment Variables:** Add these to your `~/.bashrc`.
```bash
export PATH=/usr/local/cuda-13.1/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-13.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
export CUDA_HOME=/usr/local/cuda-13.1

```



---

## üêç Phase 2: Python Virtual Environment

We keep the ML dependencies isolated to prevent breaking system-wide packages.

```bash
mkdir ~/dev/3d-teeth-reconstruction
cd ~/dev/3d-teeth-reconstruction
python3 -m venv venv
source venv/bin/activate

# Install PyTorch (Compiled for CUDA 12.1 - our target for the 'bypass')
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

```

---

## üõ† Phase 3: The "Surgical" PyTorch Bypass

PyTorch 12.1 will normally refuse to compile extensions using CUDA 13.1. We manually disable this hardware gatekeeper.

1. **Locate the File:**
`./venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py`
2. **Apply the Fix:**
Find the function `_check_cuda_version(compiler_name, compiler_version)`. Replace its entire content with the `pass` keyword.

```python
def _check_cuda_version(compiler_name: str, compiler_version: TorchVersion) -> None:
    """
    MANUAL BYPASS: 
    Allow CUDA 13.1 to work with PyTorch cu121.
    This prevents the RuntimeError during 'tiny-cuda-nn' compilation.
    """
    pass

```

---

## ‚ö° Phase 4: Compiling the Graphics Engine

Now we build `tiny-cuda-nn`, the engine required for real-time 3D reconstruction.

```bash
# ARCH 8.9 targets the RTX 4060 specifically
# TORCH_DONT_CHECK_COMPILER_ABI=1 ignores GCC version differences
CUDA_HOME=/usr/local/cuda-13.1 \
CUDACXX=/usr/local/cuda-13.1/bin/nvcc \
TCNN_CUDA_ARCHITECTURES=89 \
TORCH_DONT_CHECK_COMPILER_ABI=1 \
pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch --no-build-isolation

```

---

## ‚úÖ Phase 5: Verification

Run this script to confirm the GPU is talking to the newly compiled library.

```python
import torch
import tinycudann as tcnn

print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"GPU Model: {torch.cuda.get_device_name(0)}")
print(f"TCNN successfully loaded.")

```

---

### ‚ö†Ô∏è Troubleshooting Note

If you ever run `pip install --upgrade torch`, the `cpp_extension.py` file will be overwritten. You will need to re-apply the **Phase 3** bypass before compiling any new CUDA extensions.

[Installing PyTorch and CUDA on Ubuntu](https://www.youtube.com/watch?v=eVXl7ajMHAU)
This video provides a practical walkthrough of setting up a PyTorch environment on Ubuntu, which serves as the foundational baseline for the specialized 13.1 bypass we implemented.

Test using demo segmented oral images --
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
python3 TeethDreamer.py -b configs/TeethDreamer.yaml \
                       --accelerator gpu --devices 1 \
                       --test ckpt/TeethDreamer.ckpt \
                       --output ./results \
                       data.params.test_dir=example/teeth

Note:-- make sure to close any ram and gpu intensive programs(even chrome) to prevent crashing


next --
Locate the file on your machine:
The file upper.pkl should be inside your local instant-nsr-pl/datasets/ folder.
Check if it exists by running:

Bash
ls ~/dev/3d-teeth-reconstruction-from-images/instant-nsr-pl/datasets/upper.pkl
2. Edit the code to fix the path:
You need to change the line in blender.py to point to your local project folder instead of the developer's server.

Bash
nano instant-nsr-pl/datasets/blender.py

Stage 2: Turning PNGs into a 3D Mesh
Now you use the instant-nsr-pl folder. This is the part that uses the tiny-cuda-nn engine we worked so hard to compile. It will "shrink-wrap" a 3D shape around those PNGs.

Run this command from your project root:

Bash
cd instant-nsr-pl

sed -i '1i #include <thrust/functional.h>' *.cu
sed -i 's/cub::Equality()/thrust::equal_to<int>()/g' *.cu
sed -i 's/max_train_num_rays: 8192/max_train_num_rays: 2048/g' configs/neus-blender*.yaml
sed -i 's/ray_chunk: 4096/ray_chunk: 2048/g' configs/neus-blender*.yaml
# Reconstruct the mesh
# --img points to one of the generated PNGs in your results folder
python run.py --img ../results/<name of upper/lower generated image>.png \
              --cpu $(nproc) \
              --dir ../results/reconstruction/ \
              --normal \
              --rembg







              

We have intensively borrow codes from the following repositories. Many thanks to the authors for sharing their codes.

- [stable diffusion](https://github.com/CompVis/stable-diffusion)
- [Zero123](https://github.com/cvlab-columbia/zero123)
- [SyncDreamer](https://github.com/liuyuan-pal/SyncDreamer)
- [Wonder3D](https://github.com/xxlong0/Wonder3D)
- [Instant-nsr-pl](https://github.com/bennyguo/instant-nsr-pl)
- [Segment-Anything](https://github.com/facebookresearch/segment-anything)

## Contact information
If there are any problems, please contact by the e-mail. (xuchf2023@shanghaitech.edu.cn)

## Citation
If you find this repository useful in your project, please cite the following work. :)
```
@InProceedings{10.1007/978-3-031-72104-5_68,
author="Xu, Chenfan and Liu, Zhentao and Liu, Yuan and Dou, Yulong and Wu, Jiamin and Wang, Jiepeng and Wang, Minjiao and Shen, Dinggang and Cui, Zhiming",
editor="Linguraru, Marius George and Dou, Qi and Feragen, Aasa and Giannarou, Stamatia and Glocker, Ben and Lekadir, Karim and Schnabel, Julia A.",
title="TeethDreamer: 3D Teeth Reconstruction from¬†Five Intra-Oral Photographs",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="712--721",
isbn="978-3-031-72104-5"
}
```
